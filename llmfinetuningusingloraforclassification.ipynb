{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import zipfile\nfrom pathlib import Path\n\n# Folder to download and extract competition files\ndownload_path = Path(\"/kaggle/working/llm_competition\")\ndownload_path.mkdir(exist_ok=True, parents=True)\n\n# Step 1: Download the competition data\n!kaggle competitions download -c llm-classification-finetuning -p {download_path} --force\n\n# Step 2: Unzip all zip files in that folder\nfor zip_file in download_path.glob(\"*.zip\"):\n    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n        zip_ref.extractall(download_path)\n\n# Step 3: Set paths to train and test CSV\ntrain_file = download_path / \"train.csv\"\ntest_file  = download_path / \"test.csv\"\n\n# Step 4: Quick check\nprint(\"Train file exists:\", train_file.exists())\nprint(\"Test file exists:\", test_file.exists())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:50:30.687113Z","iopub.execute_input":"2026-01-10T19:50:30.687833Z","iopub.status.idle":"2026-01-10T19:50:31.092286Z","shell.execute_reply.started":"2026-01-10T19:50:30.687792Z","shell.execute_reply":"2026-01-10T19:50:31.091546Z"}},"outputs":[{"name":"stdout","text":"Traceback (most recent call last):\n  File \"/usr/local/bin/kaggle\", line 4, in <module>\n    from kaggle.cli import main\n  File \"/usr/local/lib/python3.12/dist-packages/kaggle/__init__.py\", line 6, in <module>\n    api.authenticate()\n  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 434, in authenticate\n    raise IOError('Could not find {}. Make sure it\\'s located in'\nOSError: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\nTrain file exists: False\nTest file exists: False\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install -q transformers datasets accelerate peft\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom peft import LoraConfig, get_peft_model, TaskType\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:49:56.470552Z","iopub.execute_input":"2026-01-10T19:49:56.471327Z","iopub.status.idle":"2026-01-10T19:50:27.767317Z","shell.execute_reply.started":"2026-01-10T19:49:56.471288Z","shell.execute_reply":"2026-01-10T19:50:27.766506Z"}},"outputs":[{"name":"stderr","text":"2026-01-10 19:50:14.238695: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768074614.450668      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768074614.506823      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768074614.955264      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768074614.955303      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768074614.955306      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768074614.955308      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\ntrain_file = \"/kaggle/input/llm-classification-finetuning/train.csv\"\ntest_file  = \"/kaggle/input/llm-classification-finetuning/test.csv\"\n\ntrain_df = pd.read_csv(train_file)\ntest_df  = pd.read_csv(test_file)\n\n# Convert one-hot winner columns into a single integer label for classification\ntrain_df[\"label\"] = train_df.apply(\n    lambda row: 0 if row.winner_model_a == 1 else 1 if row.winner_model_b == 1 else 2,\n    axis=1\n)\n\ntrain_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:51:49.943422Z","iopub.execute_input":"2026-01-10T19:51:49.944033Z","iopub.status.idle":"2026-01-10T19:51:53.260713Z","shell.execute_reply.started":"2026-01-10T19:51:49.944002Z","shell.execute_reply":"2026-01-10T19:51:53.260067Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"       id             model_a              model_b  \\\n0   30192  gpt-4-1106-preview           gpt-4-0613   \n1   53567           koala-13b           gpt-4-0613   \n2   65089  gpt-3.5-turbo-0613       mistral-medium   \n3   96401    llama-2-13b-chat  mistral-7b-instruct   \n4  198779           koala-13b   gpt-3.5-turbo-0314   \n\n                                              prompt  \\\n0  [\"Is it morally right to try to have a certain...   \n1  [\"What is the difference between marriage lice...   \n2  [\"explain function calling. how would you call...   \n3  [\"How can I create a test set for a very rare ...   \n4  [\"What is the best way to travel from Tel-Aviv...   \n\n                                          response_a  \\\n0  [\"The question of whether it is morally right ...   \n1  [\"A marriage license is a legal document that ...   \n2  [\"Function calling is the process of invoking ...   \n3  [\"Creating a test set for a very rare category...   \n4  [\"The best way to travel from Tel Aviv to Jeru...   \n\n                                          response_b  winner_model_a  \\\n0  [\"As an AI, I don't have personal beliefs or o...               1   \n1  [\"A marriage license and a marriage certificat...               0   \n2  [\"Function calling is the process of invoking ...               0   \n3  [\"When building a classifier for a very rare c...               1   \n4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n\n   winner_model_b  winner_tie  label  \n0               0           0      0  \n1               1           0      1  \n2               0           1      2  \n3               0           0      0  \n4               1           0      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"print(\"Train shape:\", train_df.shape)\nprint(\"Test shape:\", test_df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:54:06.251888Z","iopub.execute_input":"2026-01-10T19:54:06.252513Z","iopub.status.idle":"2026-01-10T19:54:06.256825Z","shell.execute_reply.started":"2026-01-10T19:54:06.252483Z","shell.execute_reply":"2026-01-10T19:54:06.256000Z"}},"outputs":[{"name":"stdout","text":"Train shape: (57477, 10)\nTest shape: (3, 4)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"class PreferenceDataset(Dataset):\n    def __init__(self, df, tokenizer, train=True, swap=True, max_len=512):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.train = train\n        self.swap = swap\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.df) * (2 if self.swap else 1)\n\n    def __getitem__(self, idx):\n        swap_flag = False\n        if self.swap and idx >= len(self.df):\n            swap_flag = True\n            idx -= len(self.df)\n        row = self.df.iloc[idx]\n\n        if swap_flag:\n            a, b = row[\"response_b\"], row[\"response_a\"]\n            label = 1 if row[\"label\"]==0 else 0 if row[\"label\"]==1 else 2\n        else:\n            a, b = row[\"response_a\"], row[\"response_b\"]\n            label = row[\"label\"]\n\n        text = f\"<PROMPT>{row['prompt']}</PROMPT><ANSWER_A>{a}</ANSWER_A><ANSWER_B>{b}</ANSWER_B>\"\n        enc = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        item = {k: v.squeeze(0) for k,v in enc.items()}\n        if self.train:\n            item[\"labels\"] = torch.tensor(label)\n        return item\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:55:21.863332Z","iopub.execute_input":"2026-01-10T19:55:21.863911Z","iopub.status.idle":"2026-01-10T19:55:21.871373Z","shell.execute_reply.started":"2026-01-10T19:55:21.863880Z","shell.execute_reply":"2026-01-10T19:55:21.870439Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"MODEL_NAME = \"microsoft/deberta-v3-base\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nbase_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    target_modules=[\"query_proj\", \"value_proj\"],\n    bias=\"none\"\n)\nmodel = get_peft_model(base_model, lora_config)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(DEVICE)\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:55:37.475875Z","iopub.execute_input":"2026-01-10T19:55:37.476609Z","iopub.status.idle":"2026-01-10T19:55:43.852858Z","shell.execute_reply.started":"2026-01-10T19:55:37.476578Z","shell.execute_reply":"2026-01-10T19:55:43.852152Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54128a9bc0d74883866f4b57b1e0735b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0059b8a7eac4bbe9dbdf427a1837a98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2df48d3bb0ed45aeade68bbcaad5b16f"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01a950e8794c401bbe534d51c00a15ae"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4f6ec20d2294fa5be456b78ee3a6947"}},"metadata":{}},{"name":"stdout","text":"trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"BATCH_SIZE = 8\nMAX_LEN = 512\n\ntrain_ds = PreferenceDataset(train_df, tokenizer, train=True, swap=True, max_len=MAX_LEN)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_ds = PreferenceDataset(test_df, tokenizer, train=False, swap=False, max_len=MAX_LEN)\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:56:08.861940Z","iopub.execute_input":"2026-01-10T19:56:08.862528Z","iopub.status.idle":"2026-01-10T19:56:08.867487Z","shell.execute_reply.started":"2026-01-10T19:56:08.862497Z","shell.execute_reply":"2026-01-10T19:56:08.866594Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"EPOCHS = 2\nLR = 2e-4\n\noptimizer = AdamW(model.parameters(), lr=LR)\nscaler = torch.cuda.amp.GradScaler()\n\nmodel.train()\nfor epoch in range(EPOCHS):\n    total_loss = 0\n    for batch in train_loader:\n        batch = {k:v.to(DEVICE) for k,v in batch.items()}\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast():\n            outputs = model(**batch)\n            loss = outputs.loss\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T19:56:20.034968Z","iopub.execute_input":"2026-01-10T19:56:20.035506Z","iopub.status.idle":"2026-01-10T23:28:49.186520Z","shell.execute_reply.started":"2026-01-10T19:56:20.035475Z","shell.execute_reply":"2026-01-10T23:28:49.185646Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_55/259342736.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n/tmp/ipykernel_55/259342736.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Loss: 1.0729\nEpoch 2 | Loss: 1.0282\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass MyDataset(Dataset):\n    def __init__(self, dataframe, is_test=False):\n        self.df = dataframe\n        self.is_test = is_test\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        a, b = row[\"response_a\"], row[\"response_b\"]\n        text = f\"<PROMPT>{row['prompt']}</PROMPT><ANSWER_A>{a}</ANSWER_A><ANSWER_B>{b}</ANSWER_B>\"\n\n        item = {\"input_text\": text}\n\n        if not self.is_test:\n            # Only include labels if not test data\n            item[\"labels\"] = row[\"label\"]\n\n        return item\n\n    def __len__(self):\n        return len(self.df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T23:32:52.416284Z","iopub.execute_input":"2026-01-10T23:32:52.416565Z","iopub.status.idle":"2026-01-10T23:32:52.421501Z","shell.execute_reply.started":"2026-01-10T23:32:52.416544Z","shell.execute_reply":"2026-01-10T23:32:52.420780Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"test_dataset = MyDataset(test_df, is_test=True)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=16,  # adjust as needed\n    shuffle=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T23:33:03.733452Z","iopub.execute_input":"2026-01-10T23:33:03.733745Z","iopub.status.idle":"2026-01-10T23:33:03.737842Z","shell.execute_reply.started":"2026-01-10T23:33:03.733720Z","shell.execute_reply":"2026-01-10T23:33:03.737073Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TestDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length=512):\n        self.df = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        a, b = row[\"response_a\"], row[\"response_b\"]\n        text = f\"<PROMPT>{row['prompt']}</PROMPT><ANSWER_A>{a}</ANSWER_A><ANSWER_B>{b}</ANSWER_B>\"\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        item = {k: v.squeeze(0) for k, v in encoding.items()}\n        return item\n\n    def __len__(self):\n        return len(self.df)\n\ntest_dataset = TestDataset(test_df, tokenizer, max_length=512)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\nmodel.eval()\npreds = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n        with torch.amp.autocast(\"cuda\"):\n            outputs = model(**batch)\n            logits = outputs.logits\n            probs = torch.softmax(logits, dim=-1)\n        preds.append(probs.cpu().numpy())\n\npreds = np.vstack(preds)\npred_labels = np.argmax(preds, axis=1)\n\nprint(\"Predictions shape:\", preds.shape)\nprint(\"Predicted labels shape:\", pred_labels.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T23:36:25.053479Z","iopub.execute_input":"2026-01-10T23:36:25.053790Z","iopub.status.idle":"2026-01-10T23:36:25.165916Z","shell.execute_reply.started":"2026-01-10T23:36:25.053764Z","shell.execute_reply":"2026-01-10T23:36:25.165123Z"}},"outputs":[{"name":"stdout","text":"Predictions shape: (3, 3)\nPredicted labels shape: (3,)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"id\": test_df[\"id\"],\n    \"winner_model_a\": preds[:,0],\n    \"winner_model_b\": preds[:,1],\n    \"winner_model_tie\": preds[:,2]\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T23:36:49.704985Z","iopub.execute_input":"2026-01-10T23:36:49.705463Z","iopub.status.idle":"2026-01-10T23:36:49.719277Z","shell.execute_reply.started":"2026-01-10T23:36:49.705424Z","shell.execute_reply":"2026-01-10T23:36:49.718582Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"        id  winner_model_a  winner_model_b  winner_model_tie\n0   136060        0.314745        0.521483          0.163772\n1   211333        0.286959        0.452637          0.260404\n2  1233961        0.369682        0.404335          0.225982","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_model_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>0.314745</td>\n      <td>0.521483</td>\n      <td>0.163772</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>0.286959</td>\n      <td>0.452637</td>\n      <td>0.260404</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>0.369682</td>\n      <td>0.404335</td>\n      <td>0.225982</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}